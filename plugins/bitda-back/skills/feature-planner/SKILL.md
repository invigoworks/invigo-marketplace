---
name: feature-planner
description: Creates phase-based feature plans with quality gates and incremental delivery structure. Use when planning features, organizing work, breaking down tasks, creating roadmaps, or structuring development strategy. Keywords: plan, planning, phases, breakdown, strategy, roadmap, organize, structure, outline.
---

# Feature Planner

## Purpose
Generate structured, phase-based plans where:
- Each phase delivers complete, runnable functionality
- Quality gates enforce validation before proceeding
- User approves plan before any work begins
- Progress tracked via markdown checkboxes
- Each phase is 1-4 hours maximum

## Planning Workflow

### Step 0: Load Project Context (MANDATORY)
**CRITICAL**: í”Œë˜ë‹ ì‹œì‘ ì „ ë°˜ë“œì‹œ CLAUDE.mdë¥¼ ì½ì–´ì„œ í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë©”ëª¨ë¦¬ì— ë¡œë“œí•œë‹¤.

1. **CLAUDE.md ì½ê¸°**: `CLAUDE.md` íŒŒì¼ì„ Read ë„êµ¬ë¡œ ì½ì–´ì„œ ì „ì²´ ë‚´ìš©ì„ íŒŒì•…
2. **í•µì‹¬ ì›ì¹™ í™•ì¸**: ì•„í‚¤í…ì²˜ í•µì‹¬ ì›ì¹™, ê³„ì¸µ êµ¬ì¡°, ë„¤ì´ë° ì»¨ë²¤ì…˜ ìˆ™ì§€
3. **ê´€ë ¨ ì‹œí–‰ë ¹ ì‹ë³„**: ê¸°ëŠ¥ê³¼ ê´€ë ¨ëœ ì‹œí–‰ë ¹ ë¬¸ì„œ(docs/standards/)ê°€ ìˆë‹¤ë©´ í•¨ê»˜ ì½ê¸°
4. **E2E í…ŒìŠ¤íŠ¸ í—Œë²• í™•ì¸**: í…ŒìŠ¤íŠ¸ ì‘ì„±ì´ í¬í•¨ëœ í”Œëœì´ë¼ë©´ Â§6.2 E2E í…ŒìŠ¤íŠ¸ í—Œë²• ìˆ™ì§€

> âš ï¸ ì´ ë‹¨ê³„ë¥¼ ê±´ë„ˆë›°ë©´ í”„ë¡œì íŠ¸ ì•„í‚¤í…ì²˜ì™€ ë§ì§€ ì•ŠëŠ” í”Œëœì´ ìƒì„±ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### Step 1: Requirements Analysis
1. Read relevant files to understand codebase architecture
2. Identify dependencies and integration points
3. Assess complexity and risks
4. Determine appropriate scope (small/medium/large)

### Step 2: Phase Breakdown with TDD Integration
Break feature into 3-7 phases where each phase:
- **Test-First**: Write tests BEFORE implementation
- Delivers working, testable functionality
- Takes 1-4 hours maximum
- Follows Red-Green-Refactor cycle
- Has measurable test coverage requirements
- Can be rolled back independently
- Has clear success criteria

**Phase Structure**:
- Phase Name: Clear deliverable
- Goal: What working functionality this produces
- **Test Strategy**: What test types, coverage target, test scenarios
- Tasks (ordered by TDD workflow):
  1. **RED Tasks**: Write failing tests first
  2. **GREEN Tasks**: Implement minimal code to make tests pass
  3. **REFACTOR Tasks**: Improve code quality while tests stay green
- Quality Gate: TDD compliance + validation criteria
- Dependencies: What must exist before starting
- **Coverage Target**: Specific percentage or checklist for this phase

### Step 2.5: Domain Classification & Naming Convention

í”Œëœ íŒŒì¼ì€ ë„ë©”ì¸ë³„ ì„œë¸Œí´ë” ì•„ë˜ `ready/` ë””ë ‰í† ë¦¬ì— ìƒì„±í•œë‹¤.

#### íŒŒì¼ëª… í˜•ì‹

**í•„ìˆ˜ í˜•ì‹**: `PLAN_{domain}-{feature}.md` (kebab-case)

| êµ¬ì„± ìš”ì†Œ | ì„¤ëª… | ì˜ˆì‹œ |
|-----------|------|------|
| `{domain}` | ê¸°ëŠ¥ì´ ì†í•˜ëŠ” ë„ë©”ì¸ (ë‹¨ìˆ˜í˜•) | `user`, `partner`, `subscription` |
| `{feature}` | êµ¬í˜„í•  ê¸°ëŠ¥ (kebab-case) | `authentication`, `excel-export`, `net-growth-statistics` |

**ì˜¬ë°”ë¥¸ ì˜ˆì‹œ**:
- `PLAN_user-authentication.md`
- `PLAN_partner-excel-export.md`
- `PLAN_subscription-net-growth-statistics.md`

**ì˜ëª»ëœ ì˜ˆì‹œ**:
- `PLAN_authentication.md` (ë„ë©”ì¸ ëˆ„ë½)
- `PLAN_user_authentication.md` (snake_case ì‚¬ìš©)
- `PLAN_UserAuthentication.md` (PascalCase ì‚¬ìš©)

#### ì „ì²´ ê²½ë¡œ í˜•ì‹

**ê²½ë¡œ í˜•ì‹**: `docs/plans/{domain}/ready/PLAN_{domain}-{feature}.md`

**ë„ë©”ì¸ ê²°ì • ë°©ë²•**:
1. ì½”ë“œë² ì´ìŠ¤ì˜ ë„ë©”ì¸ íŒ¨í‚¤ì§€ êµ¬ì¡°ë¥¼ í™•ì¸í•œë‹¤ (`modules/domain/src/**/domain/*/`)
2. ê¸°ëŠ¥ì´ ì†í•˜ëŠ” ë„ë©”ì¸ì„ ì‹ë³„í•œë‹¤
3. ë„ë©”ì¸ì„ ëª…í™•íˆ ê²°ì •í•  ìˆ˜ ì—†ëŠ” ê²½ìš° AskUserQuestionìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ì§ˆë¬¸í•œë‹¤

**ì˜ˆì‹œ**:
```
docs/plans/
â”œâ”€â”€ subscription/
â”‚   â””â”€â”€ ready/
â”‚       â””â”€â”€ PLAN_subscription-net-growth-statistics.md
â”œâ”€â”€ user/
â”‚   â””â”€â”€ ready/
â”‚       â””â”€â”€ PLAN_user-authentication.md
â””â”€â”€ warehouse/
    â””â”€â”€ ready/
        â””â”€â”€ PLAN_warehouse-inventory-sync.md
```

#### ì—°ê´€ ë¦¬ì†ŒìŠ¤ ë„¤ì´ë° (feature-impl, branch-review ì—°ë™)

í”Œëœ íŒŒì¼ëª…ì—ì„œ `PLAN_` prefixì™€ `.md` suffixë¥¼ ì œê±°í•œ ì´ë¦„ì´ ì¼ê´€ë˜ê²Œ ì‚¬ìš©ëœë‹¤:

| ë¦¬ì†ŒìŠ¤ | ë„¤ì´ë° íŒ¨í„´ | ì˜ˆì‹œ |
|--------|------------|------|
| í”Œëœ íŒŒì¼ | `PLAN_{domain}-{feature}.md` | `PLAN_user-authentication.md` |
| ë¸Œëœì¹˜ | `feature/{domain}-{feature}` | `feature/user-authentication` |
| ì›Œí¬íŠ¸ë¦¬ | `../worktrees/feature/{domain}-{feature}` | `../worktrees/feature/user-authentication` |
| ë¦¬ë·° íŒŒì¼ | `REVIEW_{domain}-{feature}.md` | `REVIEW_user-authentication.md` |

### Step 3: Plan Document Creation
Use plan-template.md to generate: `docs/plans/<domain>/ready/PLAN_<feature-name>.md`

Include:
- Overview and objectives
- Architecture decisions with rationale
- Complete phase breakdown with checkboxes
- Quality gate checklists
- Risk assessment table
- Rollback strategy per phase
- Progress tracking section
- Notes & learnings area

### Step 4: User Approval
**CRITICAL**: Use AskUserQuestion to get explicit approval before proceeding.

Ask:
- "Does this phase breakdown make sense for your project?"
- "Any concerns about the proposed approach?"
- "Should I proceed with creating the plan document?"

Only create plan document after user confirms approval.

### Step 5: Document Generation
1. Create `docs/plans/<domain>/ready/` directory if not exists
2. Generate plan document with all checkboxes unchecked
3. Add clear instructions in header about quality gates
4. Inform user of plan location and next steps

## Quality Gate Standards

Each phase MUST validate these items before proceeding to next phase:

**Build & Compilation**:
- [ ] Project builds/compiles without errors
- [ ] No syntax errors

**Test-Driven Development (TDD)**:
- [ ] Tests written BEFORE production code
- [ ] Red-Green-Refactor cycle followed
- [ ] Unit tests: â‰¥80% coverage for business logic
- [ ] Integration tests: Critical user flows validated
- [ ] Test suite runs in acceptable time (<5 minutes)

**Testing**:
- [ ] All existing tests pass
- [ ] New tests added for new functionality
- [ ] Test coverage maintained or improved

**Code Quality**:
- [ ] Linting passes with no errors
- [ ] Type checking passes (if applicable)
- [ ] Code formatting consistent

**Functionality**:
- [ ] Manual testing confirms feature works
- [ ] No regressions in existing functionality
- [ ] Edge cases tested

**Security & Performance**:
- [ ] No new security vulnerabilities
- [ ] No performance degradation
- [ ] Resource usage acceptable

**Documentation**:
- [ ] Code comments updated
- [ ] Documentation reflects changes

## Progress Tracking Protocol

Add this to plan document header:

```markdown
**CRITICAL INSTRUCTIONS**: After completing each phase:
1. âœ… Check off completed task checkboxes
2. ğŸ§ª Run all quality gate validation commands
3. âš ï¸ Verify ALL quality gate items pass
4. ğŸ“… Update "Last Updated" date
5. ğŸ“ Document learnings in Notes section
6. â¡ï¸ Only then proceed to next phase

â›” DO NOT skip quality gates or proceed with failing checks
```

## Phase Sizing Guidelines

**Small Scope** (2-3 phases, 3-6 hours total):
- Single component or simple feature
- Minimal dependencies
- Clear requirements
- Example: Add dark mode toggle, create new form component

**Medium Scope** (4-5 phases, 8-15 hours total):
- Multiple components or moderate feature
- Some integration complexity
- Database changes or API work
- Example: User authentication system, search functionality

**Large Scope** (6-7 phases, 15-25 hours total):
- Complex feature spanning multiple areas
- Significant architectural impact
- Multiple integrations
- Example: AI-powered search with embeddings, real-time collaboration

## Risk Assessment

Identify and document:
- **Technical Risks**: API changes, performance issues, data migration
- **Dependency Risks**: External library updates, third-party service availability
- **Timeline Risks**: Complexity unknowns, blocking dependencies
- **Quality Risks**: Test coverage gaps, regression potential

For each risk, specify:
- Probability: Low/Medium/High
- Impact: Low/Medium/High
- Mitigation Strategy: Specific action steps

## Rollback Strategy

For each phase, document how to revert changes if issues arise.
Consider:
- What code changes need to be undone
- Database migrations to reverse (if applicable)
- Configuration changes to restore
- Dependencies to remove

## Test Specification Guidelines

### Test-First Development Workflow

**For Each Feature Component**:
1. **Specify Test Cases** (before writing ANY code)
   - What inputs will be tested?
   - What outputs are expected?
   - What edge cases must be handled?
   - What error conditions should be tested?

2. **Write Tests** (Red Phase)
   - Write tests that WILL fail
   - Verify tests fail for the right reason
   - Run tests to confirm failure
   - Commit failing tests to track TDD compliance

3. **Implement Code** (Green Phase)
   - Write minimal code to make tests pass
   - Run tests frequently (every 2-5 minutes)
   - Stop when all tests pass
   - No additional functionality beyond tests

4. **Refactor** (Blue Phase)
   - Improve code quality while tests remain green
   - Extract duplicated logic
   - Improve naming and structure
   - Run tests after each refactoring step
   - Commit when refactoring complete

### Test Types

**Unit Tests**:
- **Target**: Individual functions, methods, classes
- **Dependencies**: None or mocked/stubbed
- **Speed**: Fast (<100ms per test)
- **Isolation**: Complete isolation from external systems
- **Coverage**: â‰¥80% of business logic

**Integration Tests**:
- **Target**: Interaction between components/modules
- **Dependencies**: May use real dependencies
- **Speed**: Moderate (<1s per test)
- **Isolation**: Tests component boundaries
- **Coverage**: Critical integration points

**End-to-End (E2E) Tests**:
- **Target**: Complete user workflows
- **Dependencies**: Real or near-real environment
- **Speed**: Slow (seconds to minutes)
- **Isolation**: Full system integration
- **Coverage**: Critical user journeys

### Test Coverage Calculation

**Coverage Thresholds** (adjust for your project):
- **Business Logic**: â‰¥90% (critical code paths)
- **Data Access Layer**: â‰¥80% (repositories, DAOs)
- **API/Controller Layer**: â‰¥70% (endpoints)
- **UI/Presentation**: Integration tests preferred over coverage

**Coverage Commands by Ecosystem**:
```bash
# JavaScript/TypeScript
jest --coverage
nyc report --reporter=html

# Python
pytest --cov=src --cov-report=html
coverage report

# Java
mvn jacoco:report
gradle jacocoTestReport

# Go
go test -cover ./...
go tool cover -html=coverage.out

# .NET
dotnet test /p:CollectCoverage=true /p:CoverageReporter=html
reportgenerator -reports:coverage.xml -targetdir:coverage

# Ruby
bundle exec rspec --coverage
open coverage/index.html

# PHP
phpunit --coverage-html coverage
```

### Common Test Patterns

**Arrange-Act-Assert (AAA) Pattern**:
```
test 'description of behavior':
  // Arrange: Set up test data and dependencies
  input = createTestData()

  // Act: Execute the behavior being tested
  result = systemUnderTest.method(input)

  // Assert: Verify expected outcome
  assert result == expectedOutput
```

**Given-When-Then (BDD Style)**:
```
test 'feature should behave in specific way':
  // Given: Initial context/state
  given userIsLoggedIn()

  // When: Action occurs
  when userClicksButton()

  // Then: Observable outcome
  then shouldSeeConfirmation()
```

**Mocking/Stubbing Dependencies**:
```
test 'component should call dependency':
  // Create mock/stub
  mockService = createMock(ExternalService)
  component = new Component(mockService)

  // Configure mock behavior
  when(mockService.method()).thenReturn(expectedData)

  // Execute and verify
  component.execute()
  verify(mockService.method()).calledOnce()
```

### Test Documentation in Plan

**In each phase, specify**:
1. **Test File Location**: Exact path where tests will be written
2. **Test Scenarios**: List of specific test cases
3. **Expected Failures**: What error should tests show initially?
4. **Coverage Target**: Percentage for this phase
5. **Dependencies to Mock**: What needs mocking/stubbing?
6. **Test Data**: What fixtures/factories are needed?

## Supporting Files Reference
- [plan-template.md](plan-template.md) - Complete plan document template


# Implementation Plan: [Feature Name]

**Status**: ğŸ”„ In Progress
**Started**: YYYY-MM-DD
**Last Updated**: YYYY-MM-DD
**Estimated Completion**: YYYY-MM-DD

---

**âš ï¸ CRITICAL INSTRUCTIONS**: After completing each phase:
1. âœ… Check off completed task checkboxes
2. ğŸ§ª Run all quality gate validation commands
3. âš ï¸ Verify ALL quality gate items pass
4. ğŸ“… Update "Last Updated" date above
5. ğŸ“ Document learnings in Notes section
6. â¡ï¸ Only then proceed to next phase

â›” **DO NOT skip quality gates or proceed with failing checks**

---

## ğŸ“‹ Overview

### Feature Description
[What this feature does and why it's needed]

### Success Criteria
- [ ] Criterion 1
- [ ] Criterion 2
- [ ] Criterion 3

### User Impact
[How this benefits users or improves the product]

---

## ğŸ—ï¸ Architecture Decisions

| Decision | Rationale | Trade-offs |
|----------|-----------|------------|
| [Decision 1] | [Why this approach] | [What we're giving up] |
| [Decision 2] | [Why this approach] | [What we're giving up] |

---

## ğŸ“¦ Dependencies

### Required Before Starting
- [ ] Dependency 1: [Description]
- [ ] Dependency 2: [Description]

### External Dependencies
- Package/Library 1: version X.Y.Z
- Package/Library 2: version X.Y.Z

---

## ğŸ§ª Test Strategy

### Testing Approach
**TDD Principle**: Write tests FIRST, then implement to make them pass

### Test Pyramid for This Feature
| Test Type | Coverage Target | Purpose |
|-----------|-----------------|---------|
| **Unit Tests** | â‰¥80% | Business logic, models, core algorithms |
| **Integration Tests** | Critical paths | Component interactions, data flow |
| **E2E Tests** | Key user flows | Full system behavior validation |

### Test File Organization
```
test/
â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ [domain/business_logic]/
â”‚   â””â”€â”€ [data/models]/
â”œâ”€â”€ integration/
â”‚   â””â”€â”€ [feature_name]/
â””â”€â”€ e2e/
    â””â”€â”€ [user_flows]/
```

### Coverage Requirements by Phase
- **Phase 1 (Foundation)**: Unit tests for core models/entities (â‰¥80%)
- **Phase 2 (Business Logic)**: Logic + repository tests (â‰¥80%)
- **Phase 3 (Integration)**: Component integration tests (â‰¥70%)
- **Phase 4 (E2E)**: End-to-end user flow test (1+ critical path)

### Test Naming Convention
Follow your project's testing framework conventions:
```
// Example structure (adapt to your framework):
describe/group: Feature or component name
  test/it: Specific behavior being tested
    // Arrange â†’ Act â†’ Assert pattern
```

---

## ğŸš€ Implementation Phases

### Phase 1: [Foundation Phase Name]
**Goal**: [Specific working functionality this phase delivers]
**Estimated Time**: X hours
**Status**: â³ Pending | ğŸ”„ In Progress | âœ… Complete

#### Tasks

**ğŸ”´ RED: Write Failing Tests First**
- [ ] **Test 1.1**: Write unit tests for [specific functionality]
  - File(s): `test/unit/[feature]/[component]_test.*`
  - Expected: Tests FAIL (red) because feature doesn't exist yet
  - Details: Test cases covering:
    - Happy path scenarios
    - Edge cases
    - Error conditions

- [ ] **Test 1.2**: Write integration tests for [component interaction]
  - File(s): `test/integration/[feature]_test.*`
  - Expected: Tests FAIL (red) because integration doesn't exist yet
  - Details: Test interaction between [list components]

**ğŸŸ¢ GREEN: Implement to Make Tests Pass**
- [ ] **Task 1.3**: Implement [component/module]
  - File(s): `src/[layer]/[component].*`
  - Goal: Make Test 1.1 pass with minimal code
  - Details: [Implementation notes]

- [ ] **Task 1.4**: Implement [integration/glue code]
  - File(s): `src/[layer]/[integration].*`
  - Goal: Make Test 1.2 pass
  - Details: [Implementation notes]

**ğŸ”µ REFACTOR: Clean Up Code**
- [ ] **Task 1.5**: Refactor for code quality
  - Files: Review all new code in this phase
  - Goal: Improve design without breaking tests
  - Checklist:
    - [ ] Remove duplication (DRY principle)
    - [ ] Improve naming clarity
    - [ ] Extract reusable components
    - [ ] Add inline documentation
    - [ ] Optimize performance if needed

#### Quality Gate âœ‹

**âš ï¸ STOP: Do NOT proceed to Phase 2 until ALL checks pass**

**TDD Compliance** (CRITICAL):
- [ ] **Red Phase**: Tests were written FIRST and initially failed
- [ ] **Green Phase**: Production code written to make tests pass
- [ ] **Refactor Phase**: Code improved while tests still pass
- [ ] **Coverage Check**: Test coverage meets requirements
  ```bash
  # Example commands (adapt to your testing framework):
  # npm test -- --coverage
  # pytest --cov=src --cov-report=html
  # dotnet test /p:CollectCoverage=true
  # go test -cover ./...

  [Your project's coverage command here]
  ```

**Build & Tests**:
- [ ] **Build**: Project builds/compiles without errors
- [ ] **All Tests Pass**: 100% of tests passing (no skipped tests)
- [ ] **Test Performance**: Test suite completes in acceptable time
- [ ] **No Flaky Tests**: Tests pass consistently (run 3+ times)

**Code Quality**:
- [ ] **Linting**: No linting errors or warnings
- [ ] **Formatting**: Code formatted per project standards
- [ ] **Type Safety**: Type checker passes (if applicable)
- [ ] **Static Analysis**: No critical issues from static analysis tools

**Security & Performance**:
- [ ] **Dependencies**: No known security vulnerabilities
- [ ] **Performance**: No performance regressions
- [ ] **Memory**: No memory leaks or resource issues
- [ ] **Error Handling**: Proper error handling implemented

**Documentation**:
- [ ] **Code Comments**: Complex logic documented
- [ ] **API Docs**: Public interfaces documented
- [ ] **README**: Usage instructions updated if needed

**Manual Testing**:
- [ ] **Functionality**: Feature works as expected
- [ ] **Edge Cases**: Boundary conditions tested
- [ ] **Error States**: Error handling verified

**Validation Commands** (customize for your project):
```bash
# Test Commands
[your test runner command]

# Coverage Check
[your coverage command]

# Code Quality
[your linter command]
[your formatter check command]
[your type checker command]

# Build Verification
[your build command]

# Security Audit
[your dependency audit command]

# Example for different ecosystems:
# JavaScript/TypeScript: npm test && npm run lint && npm run type-check
# Python: pytest && black --check . && mypy .
# Java: mvn test && mvn checkstyle:check
# Go: go test ./... && golangci-lint run
# .NET: dotnet test && dotnet format --verify-no-changes
# Ruby: bundle exec rspec && rubocop
# Rust: cargo test && cargo clippy
```

**Manual Test Checklist**:
- [ ] Test case 1: [Specific scenario to verify]
- [ ] Test case 2: [Edge case to verify]
- [ ] Test case 3: [Error handling to verify]

---

### Phase 2: [Core Feature Phase Name]
**Goal**: [Specific deliverable]
**Estimated Time**: X hours
**Status**: â³ Pending | ğŸ”„ In Progress | âœ… Complete

#### Tasks

**ğŸ”´ RED: Write Failing Tests First**
- [ ] **Test 2.1**: Write unit tests for [specific functionality]
  - File(s): `test/unit/[feature]/[component]_test.*`
  - Expected: Tests FAIL (red) because feature doesn't exist yet
  - Details: Test cases covering:
    - Happy path scenarios
    - Edge cases
    - Error conditions

- [ ] **Test 2.2**: Write integration tests for [component interaction]
  - File(s): `test/integration/[feature]_test.*`
  - Expected: Tests FAIL (red) because integration doesn't exist yet
  - Details: Test interaction between [list components]

**ğŸŸ¢ GREEN: Implement to Make Tests Pass**
- [ ] **Task 2.3**: Implement [component/module]
  - File(s): `src/[layer]/[component].*`
  - Goal: Make Test 2.1 pass with minimal code
  - Details: [Implementation notes]

- [ ] **Task 2.4**: Implement [integration/glue code]
  - File(s): `src/[layer]/[integration].*`
  - Goal: Make Test 2.2 pass
  - Details: [Implementation notes]

**ğŸ”µ REFACTOR: Clean Up Code**
- [ ] **Task 2.5**: Refactor for code quality
  - Files: Review all new code in this phase
  - Goal: Improve design without breaking tests
  - Checklist:
    - [ ] Remove duplication (DRY principle)
    - [ ] Improve naming clarity
    - [ ] Extract reusable components
    - [ ] Add inline documentation
    - [ ] Optimize performance if needed

#### Quality Gate âœ‹

**âš ï¸ STOP: Do NOT proceed to Phase 3 until ALL checks pass**

**TDD Compliance** (CRITICAL):
- [ ] **Red Phase**: Tests were written FIRST and initially failed
- [ ] **Green Phase**: Production code written to make tests pass
- [ ] **Refactor Phase**: Code improved while tests still pass
- [ ] **Coverage Check**: Test coverage meets requirements

**Build & Tests**:
- [ ] **Build**: Project builds/compiles without errors
- [ ] **All Tests Pass**: 100% of tests passing (no skipped tests)
- [ ] **Test Performance**: Test suite completes in acceptable time
- [ ] **No Flaky Tests**: Tests pass consistently (run 3+ times)

**Code Quality**:
- [ ] **Linting**: No linting errors or warnings
- [ ] **Formatting**: Code formatted per project standards
- [ ] **Type Safety**: Type checker passes (if applicable)
- [ ] **Static Analysis**: No critical issues from static analysis tools

**Security & Performance**:
- [ ] **Dependencies**: No known security vulnerabilities
- [ ] **Performance**: No performance regressions
- [ ] **Memory**: No memory leaks or resource issues
- [ ] **Error Handling**: Proper error handling implemented

**Documentation**:
- [ ] **Code Comments**: Complex logic documented
- [ ] **API Docs**: Public interfaces documented
- [ ] **README**: Usage instructions updated if needed

**Manual Testing**:
- [ ] **Functionality**: Feature works as expected
- [ ] **Edge Cases**: Boundary conditions tested
- [ ] **Error States**: Error handling verified

**Validation Commands**:
```bash
[Same as Phase 1 - customize for your project]
```

**Manual Test Checklist**:
- [ ] Test case 1: [Specific scenario to verify]
- [ ] Test case 2: [Edge case to verify]
- [ ] Test case 3: [Error handling to verify]

---

### Phase 3: [Enhancement Phase Name]
**Goal**: [Specific deliverable]
**Estimated Time**: X hours
**Status**: â³ Pending | ğŸ”„ In Progress | âœ… Complete

#### Tasks

**ğŸ”´ RED: Write Failing Tests First**
- [ ] **Test 3.1**: Write unit tests for [specific functionality]
  - File(s): `test/unit/[feature]/[component]_test.*`
  - Expected: Tests FAIL (red) because feature doesn't exist yet
  - Details: Test cases covering:
    - Happy path scenarios
    - Edge cases
    - Error conditions

- [ ] **Test 3.2**: Write integration tests for [component interaction]
  - File(s): `test/integration/[feature]_test.*`
  - Expected: Tests FAIL (red) because integration doesn't exist yet
  - Details: Test interaction between [list components]

**ğŸŸ¢ GREEN: Implement to Make Tests Pass**
- [ ] **Task 3.3**: Implement [component/module]
  - File(s): `src/[layer]/[component].*`
  - Goal: Make Test 3.1 pass with minimal code
  - Details: [Implementation notes]

- [ ] **Task 3.4**: Implement [integration/glue code]
  - File(s): `src/[layer]/[integration].*`
  - Goal: Make Test 3.2 pass
  - Details: [Implementation notes]

**ğŸ”µ REFACTOR: Clean Up Code**
- [ ] **Task 3.5**: Refactor for code quality
  - Files: Review all new code in this phase
  - Goal: Improve design without breaking tests
  - Checklist:
    - [ ] Remove duplication (DRY principle)
    - [ ] Improve naming clarity
    - [ ] Extract reusable components
    - [ ] Add inline documentation
    - [ ] Optimize performance if needed

#### Quality Gate âœ‹

**âš ï¸ STOP: Do NOT proceed until ALL checks pass**

**TDD Compliance** (CRITICAL):
- [ ] **Red Phase**: Tests were written FIRST and initially failed
- [ ] **Green Phase**: Production code written to make tests pass
- [ ] **Refactor Phase**: Code improved while tests still pass
- [ ] **Coverage Check**: Test coverage meets requirements

**Build & Tests**:
- [ ] **Build**: Project builds/compiles without errors
- [ ] **All Tests Pass**: 100% of tests passing (no skipped tests)
- [ ] **Test Performance**: Test suite completes in acceptable time
- [ ] **No Flaky Tests**: Tests pass consistently (run 3+ times)

**Code Quality**:
- [ ] **Linting**: No linting errors or warnings
- [ ] **Formatting**: Code formatted per project standards
- [ ] **Type Safety**: Type checker passes (if applicable)
- [ ] **Static Analysis**: No critical issues from static analysis tools

**Security & Performance**:
- [ ] **Dependencies**: No known security vulnerabilities
- [ ] **Performance**: No performance regressions
- [ ] **Memory**: No memory leaks or resource issues
- [ ] **Error Handling**: Proper error handling implemented

**Documentation**:
- [ ] **Code Comments**: Complex logic documented
- [ ] **API Docs**: Public interfaces documented
- [ ] **README**: Usage instructions updated if needed

**Manual Testing**:
- [ ] **Functionality**: Feature works as expected
- [ ] **Edge Cases**: Boundary conditions tested
- [ ] **Error States**: Error handling verified

**Validation Commands**:
```bash
[Same as previous phases - customize for your project]
```

**Manual Test Checklist**:
- [ ] Test case 1: [Specific scenario to verify]
- [ ] Test case 2: [Edge case to verify]
- [ ] Test case 3: [Error handling to verify]

---

## âš ï¸ Risk Assessment

| Risk | Probability | Impact | Mitigation Strategy |
|------|-------------|--------|---------------------|
| [Risk 1: e.g., API changes break integration] | Low/Med/High | Low/Med/High | [Specific mitigation steps] |
| [Risk 2: e.g., Performance degradation] | Low/Med/High | Low/Med/High | [Specific mitigation steps] |
| [Risk 3: e.g., Database migration issues] | Low/Med/High | Low/Med/High | [Specific mitigation steps] |

---

## ğŸ”„ Rollback Strategy

### If Phase 1 Fails
**Steps to revert**:
- Undo code changes in: [list files]
- Restore configuration: [specific settings]
- Remove dependencies: [if any were added]

### If Phase 2 Fails
**Steps to revert**:
- Restore to Phase 1 complete state
- Undo changes in: [list files]
- Database rollback: [if applicable]

### If Phase 3 Fails
**Steps to revert**:
- Restore to Phase 2 complete state
- [Additional cleanup steps]

---

## ğŸ“Š Progress Tracking

### Completion Status
- **Phase 1**: â³ 0% | ğŸ”„ 50% | âœ… 100%
- **Phase 2**: â³ 0% | ğŸ”„ 50% | âœ… 100%
- **Phase 3**: â³ 0% | ğŸ”„ 50% | âœ… 100%

**Overall Progress**: X% complete

### Time Tracking
| Phase | Estimated | Actual | Variance |
|-------|-----------|--------|----------|
| Phase 1 | X hours | Y hours | +/- Z hours |
| Phase 2 | X hours | - | - |
| Phase 3 | X hours | - | - |
| **Total** | X hours | Y hours | +/- Z hours |

---

## ğŸ“ Notes & Learnings

### Implementation Notes
- [Add insights discovered during implementation]
- [Document decisions that deviate from original plan]
- [Record helpful debugging discoveries]

### Blockers Encountered
- **Blocker 1**: [Description] â†’ [Resolution]
- **Blocker 2**: [Description] â†’ [Resolution]

### Improvements for Future Plans
- [What would you do differently next time?]
- [What worked particularly well?]

---

## ğŸ“š References

### Documentation
- [Link to relevant docs]
- [Link to API references]
- [Link to design mockups]

### Related Issues
- Issue #X: [Description]
- PR #Y: [Description]

---

## âœ… Final Checklist

**Before marking plan as COMPLETE**:
- [ ] All phases completed with quality gates passed
- [ ] Full integration testing performed
- [ ] Documentation updated
- [ ] Performance benchmarks meet targets
- [ ] Security review completed
- [ ] Accessibility requirements met (if UI feature)
- [ ] All stakeholders notified
- [ ] Plan document archived for future reference

---

## ğŸ“– TDD Example Workflow

### Example: Adding User Authentication Feature

**Phase 1: RED (Write Failing Tests)**

```
# Pseudocode - adapt to your testing framework

test "should validate user credentials":
  // Arrange
  authService = new AuthService(mockDatabase)
  validCredentials = {username: "user", password: "pass"}

  // Act
  result = authService.authenticate(validCredentials)

  // Assert
  expect(result.isSuccess).toBe(true)
  expect(result.user).toBeDefined()
  // TEST FAILS - AuthService doesn't exist yet
```

**Phase 2: GREEN (Minimal Implementation)**

```
class AuthService:
  function authenticate(credentials):
    // Minimal code to make test pass
    user = database.findUser(credentials.username)
    if user AND user.password == credentials.password:
      return Success(user)
    return Failure("Invalid credentials")
    // TEST PASSES - minimal functionality works
```

**Phase 3: REFACTOR (Improve Design)**

```
class AuthService:
  function authenticate(credentials):
    // Add validation
    if not this.validateCredentials(credentials):
      return Failure("Invalid input")

    // Add error handling
    try:
      user = database.findUser(credentials.username)

      // Use secure password comparison
      if user AND this.secureCompare(user.password, credentials.password):
        return Success(user)

      return Failure("Invalid credentials")
    catch DatabaseError as error:
      logger.error(error)
      return Failure("Authentication failed")
    // TESTS STILL PASS - improved code quality
```

### TDD Red-Green-Refactor Cycle Visualization

```
Phase 1: ğŸ”´ RED
â”œâ”€â”€ Write test for feature X
â”œâ”€â”€ Run test â†’ FAILS âŒ
â””â”€â”€ Commit: "Add failing test for X"

Phase 2: ğŸŸ¢ GREEN
â”œâ”€â”€ Write minimal code
â”œâ”€â”€ Run test â†’ PASSES âœ…
â””â”€â”€ Commit: "Implement X to pass tests"

Phase 3: ğŸ”µ REFACTOR
â”œâ”€â”€ Improve code quality
â”œâ”€â”€ Run test â†’ STILL PASSES âœ…
â”œâ”€â”€ Extract helper methods
â”œâ”€â”€ Run test â†’ STILL PASSES âœ…
â”œâ”€â”€ Improve naming
â”œâ”€â”€ Run test â†’ STILL PASSES âœ…
â””â”€â”€ Commit: "Refactor X for better design"

Repeat for next feature â†’
```

### Benefits of This Approach

**Safety**: Tests catch regressions immediately
**Design**: Tests force you to think about API design first
**Documentation**: Tests document expected behavior
**Confidence**: Refactor without fear of breaking things
**Quality**: Higher code coverage from day one
**Debugging**: Failures point to exact problem area

---

**Plan Status**: ğŸ”„ In Progress
**Next Action**: [What needs to happen next]
**Blocked By**: [Any current blockers] or None
